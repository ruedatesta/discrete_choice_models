{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce72a7d6",
   "metadata": {},
   "source": [
    "#  Discrete Choice Dynamic Programming\n",
    "## A Dynamic Probit Model\n",
    "\n",
    "So far, we were working in *static* problems when the agent has to decide between alternatives that do not involve a dynamic nature.\n",
    "In this lecture, we are going to cover a very simple example of Discrete Choice Dynamic Programming were time is another *state variable*. Again and for simplification, the decision space is minimal so we can focus on the innovation with respect to previous lecture: time.\n",
    "\n",
    "Assume an agent $n \\in N$ has to decide between two alternatives, working $(i=1)$ or leisure $(i=0)$. The *current* utility can be writen as\n",
    "\n",
    "$U_{n1}=\\beta x_n+\\varepsilon_{n1} $ if the agent chose to work\n",
    "\n",
    "$U_{n0}=\\mu +\\varepsilon_{n0} $ if the agent chose to do not work, i.e., leisure.\n",
    "\n",
    "However, the agent cares about future. Future is uncertain and there is no perfect forseight, so the agent has to form an *expectation*. The maximization problem of the agent can be thought as the maximization of current utility plus expected utility from tomorrow to the last day. The value function of this agent, reads\n",
    "\n",
    "$V(t,d;\\varepsilon_t) =  max \\{ U_{n1}-U_{n0} + EV_m(t+1,d;\\varepsilon_{t+1}),EV_m(t+1,d+1;\\varepsilon_{t+1})\\}$\n",
    "\n",
    "The utility of working can be written as a linear component plus an idiosyncratic shock. If an agent $n$ chooses to work, she achieves a utility that is a function of her education attainment $x$ plus a working-related shock. The parameter $\\beta$ indicates the premium for each year of education, a parameter we are interested in recover it.\n",
    "\n",
    "The utility of not-working can be written as a fixed component $\\mu$ plus a leisure-related idiosyncratic shock. The fixed component can be interpreted as unemployment insurance and for simplicity, we are assuming it does not vary between agents. This assumption can be relaxed easily.\n",
    "\n",
    "The shock $\\varepsilon_{nj}$, $j=\\{0,1\\}$ is i.i.d. and choice specific, meaning that the agent face as many shocks as decisions she can choose from. In this model, we are going to assume that these idiosyncratic shocks come from standard Normal distrubution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaaf169e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "probs (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function probs(β::Float64,μ::Float64) # Probability of working\n",
    "    Pr=fill(NaN, Tm, Tm); #Probability matrix for L=0, work\n",
    "    ϵ_th=fill(NaN, Tm, Tm); # Threshold matrix for L=0, work\n",
    "    V=fill(NaN, Tm, Tm); # Value funtion matrix\n",
    "        for t=Tm:-1:1\n",
    "            if t==Tm # Last day\n",
    "                for d=0:Tm-1\n",
    "                    ϵ_th[Tm,d+1]=-μ+P+β*(payment(d+1)-payment(d));\n",
    "                    Pr[Tm,d+1]=cdf(dist,ϵ_th[Tm,d+1]);\n",
    "                    V[Tm,d+1]=(1-Pr[Tm,d+1])*(μ-P+β*payment(d)+mean(trdist(ϵ_th[Tm,d+1],Inf)))+\n",
    "                    Pr[Tm,d+1]*(β*payment(d+1))\n",
    "                end\n",
    "            else # From day 1 to Tm-1\n",
    "                for d=0:t-1\n",
    "                    ϵ_th[t,d+1]=-μ+P+V[t+1,d+2]-V[t+1,d+1]\n",
    "                    Pr[t,d+1]=cdf(dist,ϵ_th[t,d+1])\n",
    "                    V[t,d+1]=(1-Pr[t,d+1])*(μ-P+mean(trdist(ϵ_th[t,d+1],Inf))+V[t+1,d+1])+\n",
    "                    Pr[t,d+1]*(V[t+1,d+2])\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    return Pr\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e973b3d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
